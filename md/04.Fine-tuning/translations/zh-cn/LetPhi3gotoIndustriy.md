# **让Phi-3成为行业专家**

要将 Phi-3 模型应用于某个行业，您需要向 Phi-3 模型中添加行业业务数据。我们有两种不同的选择，第一种是 RAG（检索增强生成），第二种是微调。

## **RAG vs Fine-Tuning**

### **检索增强生成**

RAG 是数据检索 + 文本生成。企业的结构化数据和非结构化数据存储在向量数据库中。在搜索相关内容时，找到相关的摘要和内容以形成上下文，并结合 LLM/SLM 的文本完成能力来生成内容。


### **微调**

微调是基于对某个模型的改进。它不需要从模型算法开始，但数据需要不断积累。如果您希望在行业应用中获得更精确的术语和语言表达，微调将是您更好的选择。但是，如果您的数据经常变化，微调可能变得复杂。

### **如何选择**

1. 如果我们的回答需要引入外部数据，RAG 是最佳选择。

2. 如果需要输出稳定且精确的行业知识，微调将是一个好选择。RAG 优先获取相关内容，但可能不总是准确把握专业细节。

3. 微调需要高质量的数据集，如果仅针对一小部分数据，可能不会产生太大差异。RAG 更具灵活性。

4. 微调像一个黑盒子，玄学，难以理解内部机制。但 RAG 可以让我们更容易找到数据来源，从而有效调整幻觉或内容错误，并提供更好的透明度。 



### **应用场景**

1. 垂直行业需要特定的专业词汇和表达, ***Fine-tuning*** 将是最佳选择。

2. 问答系统，涉及不同知识点的综合，***RAG*** 将是最佳选择。

3. 对于自动化业务流程的组合，***RAG + Fine-tuning*** 是最佳选择。

## **如何使用 RAG**

![rag](../../../../imgs/04/01/RAG.png)


向量数据库是以数学形式存储的数据集合。向量数据库使机器学习模型更容易记住以前的输入，使机器学习能够支持搜索、推荐和文本生成等用例。数据可以基于相似度度量而不是精确匹配来识别，使计算机模型能够理解数据的上下文。

向量数据库是实现 RAG 的关键。我们可以通过文本嵌入等向量模型（如 text-embedding-3，jina-ai-embedding 等）将数据转换为向量存储。

了解更多关于创建 RAG 应用程序的信息，请访问：[https://github.com/microsoft/Phi-3CookBook](https://github.com/microsoft/Phi-3CookBook?WT.mc_id=aiml-138114-kinfeylo) 


## **如何使用微调**

微调常用的算法是LoRA 和 QLoRA。如何选择呢？
- [Learn More with this sample notebook](../../code/04.Finetuning/Phi_3_Inference_Finetuning.ipynb)
- [Example of Python FineTuning Sample](../../code/04.Finetuning/FineTrainingScript.py)

### **Lora 和 QLora**

![lora](../../../../imgs/04/01/qlora.png)


LoRA（低秩适应）和 QLoRA（量化低秩适应）都是使用参数高效微调（PEFT）技术对大型语言模型（LLMs）进行微调的方法。PEFT 技术旨在比传统方法更高效地训练模型。

LoRA 是一种独立的微调技术，通过对权重更新矩阵应用低秩近似来减少内存占用。它提供了快速的训练时间，并保持接近传统微调方法的性能。

QLoRA 是 LoRA 的扩展版本，采用量化技术进一步减少内存使用。QLoRA 将预训练 LLM 中的权重参数精度量化为 4 位精度，比 LoRA 更节省内存。然而，由于额外的量化和反量化步骤，QLoRA 训练速度比 LoRA 慢约 30%。

QLoRA 使用 LoRA 作为辅助工具，修复量化过程中引入的错误。QLoRA 使得可以在相对较小的、高可用的 GPU 上微调具有数十亿参数的大型模型。例如，QLoRA 可以在2个GPU上微调一个有 70B 参数的模型，而传统方法需要 36 个 GPU 来完成。
